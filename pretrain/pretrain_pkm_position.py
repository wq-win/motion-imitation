import numpy as np  
import pickle

from tqdm import tqdm  
import matplotlib.pyplot as plt
np.random.seed(0)

# goal = [[2.4600e-02  2.4870e+00  5.6340e-01 -1.7226e+00 -9.1950e+00 -1.2318e+00
#   -5.3100e-01  2.2302e+00  4.3140e-01 -1.2960e-01 -6.3666e+00  2.5182e+00]
#  [ 9.7200e-02  2.2584e+00  1.2684e+00 -1.5066e+00 -9.0246e+00  8.3940e-01
#   -6.4020e-01  2.6814e+00  2.4960e-01 -4.9320e-01 -5.8842e+00  2.8068e+00]
#  [ 2.0460e-01  2.0958e+00  1.6968e+00 -6.1800e-01 -8.3070e+00  2.5662e+00
#   -6.8100e-01  2.8044e+00  4.6260e-01 -6.6120e-01 -5.3754e+00  3.2412e+00]
#  [ 5.4000e-02  1.9518e+00  2.0208e+00  2.6640e-01 -5.8878e+00  3.4038e+00
#   -7.2840e-01  2.6442e+00  9.0840e-01 -7.9260e-01 -4.5918e+00  3.5340e+00]
#  [-1.3920e-01  1.6278e+00  2.1864e+00  9.8640e-01 -3.9192e+00  4.3218e+00
#   -7.6680e-01  2.3736e+00  1.1874e+00 -2.8980e-01 -3.2970e+00  3.2748e+00]
#  [-2.7660e-01  1.0926e+00  2.1174e+00  1.0914e+00 -3.3468e+00  3.8262e+00
#   -7.2720e-01  1.6800e+00  1.6740e+00  2.7300e-01 -2.0298e+00  2.7738e+00]
#  [-4.3320e-01  1.0776e+00  2.4036e+00 -1.5720e-01 -2.4162e+00  5.0574e+00
#   -5.4000e-01  1.5810e+00  1.6692e+00  7.8120e-01 -4.1340e-01  2.2548e+00]
#  [-4.6080e-01  1.0434e+00  2.0772e+00  3.8766e+00 -8.5440e-01  3.5532e+00
#   -2.9160e-01  1.6032e+00  1.3104e+00  6.5520e-01  1.9482e+00  4.8000e-03]
#  [-2.9400e-01  1.4772e+00  1.7436e+00  2.9724e+00  1.4214e+00  2.1966e+00
#   -1.9680e-01  2.3178e+00 -3.5520e-01 -1.1640e-01  4.3776e+00 -3.2232e+00]
#  [-9.9600e-02  1.4496e+00  1.1340e+00  1.2396e+00  2.5866e+00  1.4910e+00
#   -2.2800e-02  3.0324e+00 -2.4654e+00  2.5860e-01  3.1902e+00 -2.1192e+00]
#  [ 8.2800e-02  1.5624e+00  5.1900e-01  9.8100e-01  2.9544e+00  1.3854e+00
#    6.6600e-01  1.9590e+00 -3.1050e+00  8.3400e-02  3.3126e+00 -2.1564e+00]
#  [ 3.7200e-01  1.4274e+00  4.8600e-01 -1.9728e+00  4.2318e+00 -4.3932e+00
#    1.4280e+00 -3.8580e-01 -2.0976e+00 -1.4220e-01  2.7030e+00 -1.2426e+00]
#  [ 3.2340e-01  1.5564e+00 -2.9400e-01 -1.6320e-01  2.7018e+00 -2.1936e+00
#    1.3296e+00 -1.6734e+00 -2.4594e+00 -2.2380e-01  2.2134e+00 -7.6440e-01]
#  [ 5.2560e-01  1.2636e+00 -1.6752e+00 -7.1280e-01  3.6948e+00 -3.0918e+00
#    6.8580e-01 -2.5644e+00 -2.9988e+00 -2.8920e-01  2.2146e+00 -6.3120e-01]
#  [ 4.7100e-01  5.0280e-01 -5.0256e+00 -8.0220e-01  3.6294e+00 -2.8212e+00
#    2.4360e-01 -2.0658e+00 -3.3690e+00 -1.0980e-01  2.2848e+00 -5.9280e-01]
#  [-1.3740e-01 -1.1988e+00 -5.7000e+00 -2.3040e-01  3.6606e+00 -2.4744e+00
#    9.5280e-01 -2.5884e+00 -2.8398e+00 -1.8900e-01  2.2098e+00 -3.7140e-01]
#  [-3.3600e-02 -1.5978e+00 -5.5140e+00  1.8120e-01  3.2496e+00 -1.6566e+00
#    1.1640e+00 -3.9390e+00 -1.8042e+00  7.8000e-03  1.8696e+00  2.5260e-01]
#  [-2.3160e-01 -2.0820e+00 -6.8700e+00  4.6740e-01  3.2904e+00 -8.8200e-01
#    5.6160e-01 -5.4150e+00  9.0000e-03  4.3200e-02  2.2620e+00  7.9800e-02]
#  [-6.7080e-01 -5.1552e+00 -5.1270e+00  5.7300e-01  3.0234e+00 -5.0400e-02
#    5.8200e-02 -5.7822e+00  1.1088e+00 -6.8400e-02  2.2314e+00  4.3500e-01]
#  [-6.3900e-01 -7.9938e+00 -2.4150e+00  4.4520e-01  2.7036e+00  4.6320e-01
#   -1.9200e-02 -5.9166e+00  1.8348e+00 -1.8000e-01  2.3040e+00  5.0160e-01]
#  [-4.7160e-01 -9.1560e+00  9.9000e-02  7.3980e-01  2.4174e+00  1.1472e+00
#   -4.8300e-01 -6.0120e+00  2.8008e+00 -2.9220e-01  2.2830e+00  7.4820e-01]
#  [-2.2500e-01 -8.5650e+00  2.1258e+00  6.6420e-01  2.2284e+00  1.5912e+00
#   -7.4340e-01 -5.8434e+00  3.4110e+00 -3.7620e-01  2.3538e+00  7.4880e-01]
#  [ 1.1700e-01 -6.8448e+00  3.8532e+00  6.7200e-01  1.7196e+00  2.2284e+00
#   -7.2540e-01 -5.2050e+00  3.8532e+00 -6.2940e-01  2.2662e+00  1.2564e+00]
#  [ 1.4400e-01 -5.0298e+00  4.5498e+00  4.6980e-01  1.4478e+00  2.4498e+00
#   -6.0660e-01 -4.1988e+00  3.5538e+00 -6.5340e-01  1.8720e+00  1.6374e+00]
#  [ 4.0800e-02 -3.9318e+00  3.8262e+00  1.5900e-01  1.0404e+00  2.5422e+00
#   -3.2520e-01 -2.8626e+00  3.4638e+00 -7.3320e-01  1.5450e+00  1.9218e+00]
#  [ 4.6380e-01 -3.1086e+00  4.6590e+00  1.0860e-01  7.9740e-01  2.9514e+00
#   -1.5360e-01 -1.7364e+00  3.3450e+00 -2.3280e-01  1.2732e+00  2.0604e+00]
#  [ 3.8238e+00 -1.2726e+00  5.4192e+00 -8.4000e-02  5.4660e-01  2.8734e+00
#    9.6780e-01  5.2440e-01  1.8474e+00 -4.2180e-01  9.5460e-01  2.1708e+00]
#  [ 2.9352e+00  1.9218e+00  3.8424e+00 -1.5000e-01  9.0540e-01  2.7792e+00
#    5.1600e-02  3.7380e+00 -1.5426e+00 -4.7940e-01  2.1042e+00 -5.1000e-02]
#  [ 6.9000e-02  2.6526e+00  1.0674e+00 -6.0000e-02  1.3716e+00  2.3532e+00
#   -3.6840e-01  3.5298e+00 -2.5542e+00 -3.4020e-01  3.3612e+00 -2.9172e+00]
#  [-2.2698e+00  4.6164e+00 -3.4404e+00  3.7200e-02  1.7274e+00  1.6224e+00
#    7.1400e-02  3.2304e+00 -1.9536e+00  5.0160e-01  3.2946e+00 -3.8874e+00]
#  [-2.6340e-01  3.3708e+00 -2.4606e+00  3.0300e-01  1.6722e+00  3.4740e-01
#   -1.3800e-01  3.4098e+00 -2.2794e+00  1.2816e+00  1.0062e+00 -3.1494e+00]
#  [ 2.5080e-01  3.0870e+00 -1.4070e+00  2.9460e-01  1.1190e+00 -8.3280e-01
#    1.4760e-01  3.0486e+00 -1.3038e+00  1.1382e+00 -1.0944e+00 -2.8848e+00]
#  [-8.6700e-01  3.4716e+00 -2.6826e+00 -8.9400e-02  7.8000e-02 -2.9436e+00
#   -1.8600e-01  2.4468e+00 -1.0368e+00  8.3340e-01 -1.4820e+00 -3.9564e+00]
#  [-5.4480e-01  3.2310e+00 -2.0118e+00 -7.1760e-01 -4.5060e-01 -6.1830e+00
#   -2.3880e-01  2.1456e+00 -5.8500e-01  6.2940e-01 -1.9518e+00 -3.9306e+00]
#  [-5.6460e-01  3.3228e+00 -1.4322e+00 -1.1994e+00 -5.1540e-01 -6.3564e+00
#   -3.5760e-01  2.5218e+00 -7.5660e-01  7.7340e-01 -2.7636e+00 -2.8074e+00]
#  [-4.3260e-01  3.1116e+00 -9.7800e-01 -2.1708e+00 -1.9572e+00 -7.1886e+00
#   -2.3760e-01  2.5554e+00 -5.6820e-01  7.7100e-01 -4.2798e+00 -1.4838e+00]
#  [-5.4060e-01  2.9040e+00 -4.2420e-01 -2.1264e+00 -5.1552e+00 -5.6112e+00
#   -4.9320e-01  2.2098e+00  1.0440e-01  4.8780e-01 -5.6184e+00  2.5200e-02]
#  [-4.4280e-01  2.6568e+00  8.2200e-02 -2.6442e+00 -8.1948e+00 -3.0912e+00
#   -5.5800e-01  1.7130e+00  6.9240e-01  4.0800e-01 -6.4176e+00  1.5420e+00]
#  [ 3.8400e-02 -1.2840e+00 -2.8320e-01  5.9880e-01  5.0052e+00 -9.8820e-01
#    2.4318e+00  2.0820e-01  1.5720e-01 -1.0830e+00 -3.8694e+00  2.3808e+00]]

pace = [
  [0.00000, 0.00000, 0.43701, 0.49491, 0.53393, 0.49912, 0.46997, -0.12721, 0.07675, -0.95545, -0.25301, 0.18682, -1.14403, -0.19362, 0.14030, -0.77823, -0.09528, 0.05437, -0.97596],
  [0.01641, 0.00223, 0.43771, 0.48959, 0.53669, 0.50119, 0.47018, -0.12680, 0.11820, -0.94606, -0.28172, 0.03357, -1.16456, -0.20247, 0.17747, -0.77104, -0.09744, -0.05174, -0.93399],
  [0.03278, 0.00476, 0.43896, 0.48274, 0.53845, 0.50530, 0.47084, -0.12518, 0.15584, -0.92492, -0.30683, -0.11684, -1.15057, -0.21314, 0.22216, -0.76688, -0.10566, -0.14981, -0.88721],
  [0.04882, 0.00706, 0.44055, 0.47656, 0.53895, 0.50939, 0.47217, -0.12177, 0.19077, -0.89664, -0.31713, -0.25529, -1.10780, -0.22449, 0.26890, -0.75917, -0.11668, -0.23940, -0.83319],
  [0.06588, 0.00883, 0.44210, 0.47093, 0.53858, 0.51304, 0.47428, -0.12087, 0.22330, -0.86296, -0.31269, -0.35342, -1.05107, -0.23663, 0.31297, -0.74403, -0.12989, -0.31593, -0.77429],
  [0.08286, 0.01033, 0.44397, 0.46634, 0.53774, 0.51543, 0.47717, -0.12319, 0.25043, -0.82652, -0.29625, -0.41874, -0.97904, -0.24941, 0.35253, -0.72424, -0.13472, -0.37088, -0.71971],
  [0.09884, 0.01171, 0.44605, 0.46371, 0.53747, 0.51525, 0.48022, -0.12780, 0.26864, -0.79123, -0.27806, -0.47452, -0.91527, -0.26153, 0.38053, -0.69634, -0.13017, -0.40471, -0.67348],
  [0.11564, 0.01337, 0.44783, 0.46172, 0.53682, 0.51397, 0.48422, -0.13502, 0.28660, -0.75117, -0.28068, -0.51479, -0.83098, -0.27053, 0.40688, -0.66852, -0.11715, -0.41160, -0.63590],
  [0.13247, 0.01557, 0.44866, 0.46017, 0.53653, 0.51145, 0.48867, -0.14270, 0.30399, -0.71655, -0.21607, -0.52903, -0.77176, -0.27539, 0.43360, -0.64668, -0.10623, -0.37913, -0.63582],
  [0.14967, 0.01760, 0.44808, 0.45870, 0.53617, 0.50991, 0.49205, -0.14760, 0.32861, -0.68749, -0.16653, -0.50534, -0.73515, -0.27867, 0.47223, -0.65260, -0.10817, -0.30617, -0.68954],
  [0.16688, 0.01936, 0.44679, 0.45801, 0.53697, 0.50846, 0.49332, -0.14926, 0.35277, -0.66859, -0.14587, -0.46223, -0.71030, -0.27905, 0.52277, -0.69369, -0.10386, -0.25300, -0.72486],
  [0.18457, 0.02113, 0.44491, 0.45884, 0.53861, 0.50792, 0.49131, -0.14788, 0.37881, -0.65994, -0.12952, -0.41299, -0.68721, -0.26795, 0.55542, -0.74544, -0.10247, -0.19779, -0.76080],
  [0.20315, 0.02255, 0.44338, 0.46159, 0.54103, 0.50733, 0.48666, -0.14168, 0.40260, -0.65184, -0.16240, -0.34246, -0.76043, -0.24415, 0.54899, -0.78040, -0.10484, -0.15274, -0.78151],
  [0.22288, 0.02351, 0.44195, 0.46521, 0.54355, 0.50682, 0.48089, -0.13629, 0.42854, -0.65674, -0.16512, -0.29743, -0.79699, -0.22199, 0.52110, -0.82139, -0.10857, -0.11585, -0.79425],
  [0.24349, 0.02420, 0.44024, 0.46907, 0.54564, 0.50727, 0.47426, -0.12753, 0.44960, -0.68466, -0.17700, -0.23585, -0.84852, -0.21056, 0.47836, -0.87137, -0.11339, -0.07894, -0.80477],
  [0.26441, 0.02473, 0.43823, 0.47325, 0.54703, 0.50712, 0.46865, -0.11968, 0.45798, -0.76842, -0.19037, -0.17536, -0.89554, -0.20650, 0.44393, -0.92752, -0.11522, -0.04086, -0.81465],
  [0.28558, 0.02464, 0.43620, 0.47835, 0.54689, 0.50616, 0.46464, -0.12197, 0.43800, -0.86342, -0.19421, -0.11435, -0.93678, -0.19062, 0.40079, -0.97485, -0.11837, -0.00403, -0.82084],
  [0.30563, 0.02398, 0.43551, 0.48404, 0.54571, 0.50448, 0.46196, -0.12253, 0.41137, -0.95532, -0.19119, -0.06019, -0.96439, -0.17122, 0.33514, -1.00492, -0.11824, 0.02713, -0.81663],
  [0.32565, 0.02340, 0.43543, 0.48863, 0.54314, 0.50286, 0.46192, -0.12639, 0.37667, -1.06982, -0.18340, -0.00535, -0.97909, -0.16186, 0.24489, -1.00477, -0.11752, 0.06483, -0.81530],
  [0.34513, 0.02264, 0.43600, 0.49248, 0.53972, 0.50166, 0.46315, -0.13757, 0.29075, -1.15527, -0.17385, 0.04504, -0.97993, -0.16089, 0.14852, -0.98629, -0.11866, 0.10202, -0.80805],
  [0.36352, 0.02156, 0.43678, 0.49651, 0.53506, 0.50047, 0.46553, -0.14822, 0.15752, -1.19552, -0.16643, 0.09010, -0.97221, -0.16121, 0.04991, -0.95571, -0.12166, 0.14042, -0.79969],
  [0.38133, 0.01988, 0.43839, 0.50053, 0.52944, 0.49854, 0.46971, -0.15608, 0.00492, -1.19387, -0.15410, 0.13039, -0.95309, -0.16926, -0.05029, -0.90903, -0.12653, 0.17847, -0.78722],
  [0.39841, 0.01837, 0.44059, 0.50384, 0.52281, 0.49739, 0.47479, -0.15983, -0.13783, -1.15844, -0.14303, 0.16753, -0.92657, -0.18165, -0.14768, -0.85218, -0.13280, 0.21770, -0.77474],
  [0.41513, 0.01669, 0.44294, 0.50649, 0.51629, 0.49694, 0.47954, -0.15788, -0.25191, -1.09422, -0.13183, 0.19619, -0.88943, -0.19374, -0.23443, -0.78796, -0.14329, 0.25547, -0.75380],
  [0.43177, 0.01473, 0.44521, 0.50907, 0.51037, 0.49701, 0.48306, -0.15548, -0.33574, -1.01839, -0.12400, 0.22032, -0.84860, -0.20385, -0.30441, -0.72873, -0.15418, 0.28667, -0.72651],
  [0.44802, 0.01314, 0.44760, 0.51097, 0.50622, 0.49819, 0.48420, -0.15480, -0.40127, -0.95462, -0.12135, 0.23766, -0.80623, -0.20927, -0.35212, -0.67100, -0.16640, 0.31242, -0.69448],
  [0.46489, 0.01140, 0.45021, 0.51169, 0.50451, 0.49981, 0.48356, -0.14707, -0.45308, -0.87697, -0.11954, 0.25095, -0.75704, -0.21183, -0.38106, -0.61525, -0.17028, 0.33364, -0.66014],
  [0.48202, 0.00922, 0.45259, 0.51313, 0.50273, 0.50274, 0.48085, -0.08334, -0.47429, -0.78665, -0.12094, 0.26006, -0.70915, -0.19570, -0.37232, -0.58446, -0.17731, 0.34955, -0.62396],
  [0.49978, 0.00692, 0.45301, 0.51399, 0.49995, 0.50649, 0.47889, -0.03442, -0.44226, -0.72261, -0.12344, 0.27515, -0.66283, -0.19484, -0.31002, -0.61017, -0.18530, 0.38462, -0.62481],
  [0.51793, 0.00522, 0.45155, 0.51416, 0.49704, 0.50996, 0.47805, -0.03327, -0.39805, -0.70482, -0.12444, 0.29801, -0.62361, -0.20098, -0.25119, -0.65274, -0.19097, 0.44064, -0.67343],
  [0.53748, 0.00395, 0.44913, 0.51411, 0.49529, 0.51164, 0.47813, -0.07110, -0.32111, -0.76216, -0.12382, 0.32680, -0.59657, -0.19979, -0.19735, -0.68530, -0.18261, 0.49555, -0.73822],
  [0.55659, 0.00280, 0.44697, 0.51485, 0.49644, 0.50985, 0.47805, -0.07549, -0.26493, -0.80317, -0.11877, 0.35467, -0.59078, -0.20209, -0.14052, -0.72329, -0.16125, 0.51232, -0.79071],
  [0.57577, 0.00167, 0.44507, 0.51650, 0.49783, 0.50696, 0.47789, -0.07131, -0.21348, -0.82662, -0.11386, 0.37332, -0.60466, -0.19963, -0.08971, -0.74502, -0.14228, 0.49408, -0.83879],
  [0.59497, 0.00154, 0.44312, 0.51800, 0.50013, 0.50330, 0.47774, -0.08576, -0.15562, -0.87133, -0.11535, 0.37462, -0.65372, -0.20273, -0.04893, -0.76230, -0.12839, 0.46938, -0.90473],
  [0.61400, 0.00191, 0.44160, 0.51843, 0.50324, 0.49974, 0.47774, -0.09484, -0.10177, -0.90486, -0.12731, 0.36711, -0.75677, -0.20671, -0.01317, -0.77205, -0.11790, 0.43685, -0.97024],
  [0.63303, 0.00233, 0.44009, 0.51746, 0.50557, 0.49713, 0.47905, -0.10425, -0.04639, -0.92873, -0.14730, 0.35852, -0.86271, -0.21267, 0.02886, -0.78466, -0.10501, 0.39079, -1.01703],
  [0.65155, 0.00263, 0.43886, 0.51586, 0.50831, 0.49492, 0.48016, -0.11146, 0.00547, -0.94503, -0.18348, 0.32590, -0.98252, -0.21663, 0.07145, -0.79413, -0.09216, 0.31946, -1.04176],
  [0.67005, 0.00126, 0.43824, 0.51299, 0.51174, 0.49342, 0.48113, -0.12047, 0.05387, -0.95210, -0.21892, 0.23998, -1.07604, -0.22485, 0.10828, -0.79239, -0.08403, 0.22582, -1.04134],
  [0.68773, 0.00000, 0.43701, 0.50903, 0.51581, 0.49242, 0.48203, -0.12785, 0.09815, -0.95073, -0.26299, 0.10340, -1.12756, -0.23415, 0.13683, -0.78085, -0.07723, 0.11886, -1.01564]
]
pace = np.array(pace)

learning_rate = 1e-3
epsilon = 1-1e-5
epochs = 1000 # 迭代次数  
episode = 100
loss_list = []
# def hashing(self, data):
#     centered_data = data - np.mean(data, axis=1)[:,None]
#     y = centered_data @ self.weights
#     return y
   

# PN dims is 16
# for i in tqdm(range(episode)):
PN = pace[:, 3:]
PN_dims = len(PN[0])  
PN = PN + np.random.normal(0,abs(0.02*PN),size=PN.shape)
# KC dims is 1000
KC_dims = 1000   
activate_KC_rate = .05  # percentage of KC activit level
activate_KC_dims = int(KC_dims * activate_KC_rate)

# MBON dims is 12
MBON_hat = np.vstack((pace[1:, 7:] ,pace[:1, 7:]))
MBON_dims = len(MBON_hat[0])  
PNtoKCweight = np.random.random((PN_dims, KC_dims))

# remian PNtoKCweight top 3*1000 weight
PNtoKCweight1d = PNtoKCweight.flatten()  
top_indices = np.argpartition(PNtoKCweight1d, -3 * KC_dims)[-3 * KC_dims:] 
sorted_indices = top_indices[np.argsort(PNtoKCweight1d[top_indices])[::-1]]  
zero_array = np.zeros_like(PNtoKCweight)  
for idx in sorted_indices:  
    # 使用np.unravel_index将一维索引转换为二维索引  
    row, col = np.unravel_index(idx, PNtoKCweight.shape)  
    zero_array[row, col] = PNtoKCweight[row, col]  
PNtoKCweight = zero_array

KC = PN @ PNtoKCweight
sorted_indices = np.argsort(KC, axis=1)[:, ::-1]  
inactivate_indices = sorted_indices[:, activate_KC_dims:]  
KC[np.arange(KC.shape[0])[:, None], inactivate_indices] = 0


KCtoMBONweight = np.random.random((KC_dims, MBON_dims))

for i in range(int(epochs)):
    
    KCtoMBONweight += KC.T @ (MBON_hat - KC @ KCtoMBONweight) * learning_rate
    loss = MBON_hat - KC @ KCtoMBONweight
    loss = loss ** 2
    loss_list.append(loss.mean())

    # if i % 10000 == 0:
    learning_rate *= epsilon
    
plt.plot(range(len(loss_list)), loss_list, "g")
print("loss**2: ",loss_list[-1])
plt.show()
allresult ={'PNtoKCweight':PNtoKCweight,'PN':PN,'activate_KC_dims':activate_KC_dims,'KCtoMBONweight':KCtoMBONweight}
# 使用pickle保存数组到文件  
with open('PretrainModel/weight_data.pkl', 'wb') as f:  
    pickle.dump(allresult, f)